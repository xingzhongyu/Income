{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findAllFile(base):\n",
    "    for root, ds, fs in os.walk(base):\n",
    "        for f in fs:\n",
    "            fullname = os.path.join(root, f)\n",
    "            yield fullname"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 22792, 14) (94, 22792, 14) (20, 22792, 14) (20, 22792, 14)\n"
     ]
    }
   ],
   "source": [
    "bst_shap=[]\n",
    "clf_shap=[]\n",
    "logreg_shap=[]\n",
    "rf_shap=[]\n",
    "for i in findAllFile(\"/home/zyxing/adult_income/shaps\"):\n",
    "    name=os.path.basename(i)\n",
    "    # print(i)\n",
    "    if name.startswith(\"bst\"):\n",
    "        bst_shap.append(np.load(i))\n",
    "    elif name.startswith(\"clf\"):\n",
    "        clf_shap.append(np.load(i))\n",
    "    elif name.startswith(\"logreg\"):\n",
    "        logreg_shap.append(np.load(i))\n",
    "    elif name.startswith(\"rf\"):\n",
    "        rf_shap.append(np.load(i))\n",
    "bst_shap,clf_shap,logreg_shap,rf_shap=np.array(bst_shap),np.array(clf_shap),np.array(logreg_shap),np.array(rf_shap)\n",
    "print(bst_shap.shape,clf_shap.shape,logreg_shap.shape,rf_shap.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22792, 14)\n"
     ]
    }
   ],
   "source": [
    "all_shap_data=np.concatenate([bst_shap,clf_shap,rf_shap],axis=0)\n",
    "all_shap=np.mean(all_shap_data,axis=0)\n",
    "print(all_shap.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[19 11  8  5 15]\n",
      "(22792, 5, 14)\n"
     ]
    }
   ],
   "source": [
    "rand_ints=np.random.choice(20,5,replace=False)\n",
    "all_input_shap=logreg_shap[rand_ints,:,:]\n",
    "print(rand_ints)\n",
    "all_input_shap=all_input_shap.transpose([1,0,2])\n",
    "print(all_input_shap.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "\n",
    "class cls_dataset(Dataset):\n",
    "    def __init__(self,x_,y_):\n",
    "        self.X=torch.tensor(x_).to(torch.float)\n",
    "        self.y=torch.tensor(y_).to(torch.float)\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index],self.y[index],index\n",
    "    def __len__(self):\n",
    "        return len(self.X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(22792, 7) (22792, 9)\n",
      "(6838, 7)\n"
     ]
    }
   ],
   "source": [
    "#500 22792\n",
    "#1000 14\n",
    "from sklearn import decomposition\n",
    "from sklearn.model_selection import train_test_split\n",
    "import joblib\n",
    "pca=decomposition.PCA(n_components=0.99)\n",
    "partial_shaps_pca=pca.fit_transform(all_input_shap.reshape(22792,-1))\n",
    "joblib.dump(pca,\"partial_shaps_pca.m\")\n",
    "pca=decomposition.PCA(n_components=0.99)\n",
    "shaps_mean_pca=pca.fit_transform(all_shap.reshape(-1,14))\n",
    "joblib.dump(pca,\"shaps_mean_pca.m\")\n",
    "input_dim=partial_shaps_pca.shape[1]\n",
    "output_dim=shaps_mean_pca.shape[1]\n",
    "\n",
    "print(partial_shaps_pca.shape,shaps_mean_pca.shape)\n",
    "X_train, X_test, y_train, y_test = train_test_split(partial_shaps_pca, shaps_mean_pca, test_size = 0.3, shuffle=False)\n",
    "print(X_test.shape)\n",
    "train_datasets=cls_dataset(X_train,y_train)\n",
    "test_datasets=cls_dataset(X_test,y_test)\n",
    "train_dataloader = DataLoader(train_datasets,batch_size=128,shuffle=True)\n",
    "test_dataloader = DataLoader(test_datasets,batch_size=128,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "class pca_Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(pca_Net,self).__init__()\n",
    "        self.fc1=nn.Linear(input_dim,11)\n",
    "        self.fc2=nn.Linear(11,output_dim)\n",
    "        self.relu=nn.ReLU(inplace=True)\n",
    "    def forward(self,x):\n",
    "        x=self.relu(self.fc1(x))\n",
    "        x=self.fc2(x)\n",
    "        return x\n",
    "net=pca_Net()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished Training! Total cost time:  1.8338234424591064\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import torch.optim as optim\n",
    "criterion=nn.MSELoss()\n",
    "optimizer = optim.Adam(net.parameters(), lr=0.001)\n",
    "start = time.time()\n",
    "for epoch in range(10):\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(train_dataloader, 0):\n",
    "        # 获取输入数据\n",
    "        inputs, label,index = data\n",
    "        # 清空梯度缓存\n",
    "        optimizer.zero_grad()\n",
    "        # print(inputs.shape)\n",
    "        # print(torch.dist(inputs,torch.tensor(partial_shaps[index]),1))\n",
    "        outputs = net(inputs).squeeze()\n",
    "        label=label.squeeze()\n",
    "        # print(outputs.shape,label.shape)\n",
    "        loss = criterion(outputs, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # 打印统计信息\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:\n",
    "            # 每 2000 次迭代打印一次信息\n",
    "            print('[%d, %5d] loss: %.3f' % (epoch + 1, i+1, running_loss / 50))\n",
    "            running_loss = 0.0\n",
    "print('Finished Training! Total cost time: ', time.time()-start)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def getDIff(k:int,nums1:np.ndarray,nums2:np.ndarray,alpha=0.2,beta=0.2):\n",
    "        #排序得到K，K中交集同号数，K中交集异号数，K中非交集数\n",
    "    nums1=nums1.flatten()\n",
    "    nums2=nums2.flatten()\n",
    "    abs_nums1=np.maximum(nums1,-nums1)\n",
    "    abs_nums2=np.maximum(nums2,-nums2)\n",
    "    top_k1_index=abs_nums1.argsort()[-k:]\n",
    "    top_k2_index=abs_nums2.argsort()[-k:]\n",
    "    inter_index=np.intersect1d(top_k1_index,top_k2_index)\n",
    "    same_inter=np.sum(nums1[inter_index]*nums2[inter_index]>0)\n",
    "    score=math.exp((k-(same_inter-alpha*(len(inter_index)-same_inter)-beta*(k-len(inter_index))))*-1)\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(),\"net.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # for i,data in enumerate(test_dataloader,0):\n",
    "# #     x, y, index=data\n",
    "#     # index+=int(22792*0.7)\n",
    "#     # print(shaps_mean_pca[index]==y.detach().numpy())\n",
    "# x,y,index=next(iter(test_dataloader))\n",
    "# print(index)\n",
    "# np.where((torch.tensor(shaps_mean_pca).to(torch.float)).detach().numpy()==y.detach().numpy())\n",
    "# (torch.tensor(shaps_mean_pca).to(torch.float)).detach().numpy()[16371]\n",
    "# y.detach().numpy()\n",
    "# 417+int(22792*0.7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(481.3336, dtype=torch.float64) tensor(690.7486, dtype=torch.float64) tensor(550.8325, dtype=torch.float64) 5.144519533730801e-95 4.561331580983388e-96\n"
     ]
    }
   ],
   "source": [
    "\n",
    "loss1 = 0\n",
    "loss2 = 0\n",
    "loss3 = 0\n",
    "loss4 = 0\n",
    "loss5 = 0\n",
    "# needTo=False\n",
    "for i,data in enumerate(test_dataloader,0):\n",
    "    # print(x.shape,partial_shaps[index].shape)\n",
    "    x, y, index=data\n",
    "    index+=int(22792*0.7)\n",
    "    # print(torch.dist(x,torch.tensor(partial_shaps[index]),1))\n",
    "\n",
    "    # print(net.state_dict())\n",
    "    # print(x1.shape)\n",
    "    y=y.squeeze()\n",
    "   \n",
    "        # if needTo:\n",
    "        #     if len(index)==1:\n",
    "        #\n",
    "        #         x=torch.tensor(partial_shaps[index].transpose(1,2,0))\n",
    "        #     else:\n",
    "        #\n",
    "        #         x=torch.tensor(partial_shaps[index].transpose(0,2,3,1))\n",
    "    #需要思考\n",
    "    output_tensor=net.forward(x).squeeze()\n",
    "    output_tensor=torch.tensor(pca.inverse_transform(output_tensor.detach().numpy()))\n",
    "    \n",
    "    if len(index)==1:\n",
    "        part_mean_shap=torch.tensor(np.mean(all_input_shap[index],axis=0))\n",
    "    else:\n",
    "        part_mean_shap=torch.tensor(np.mean(all_input_shap[index],axis=1))\n",
    "    # print(part_mean_shap.shape)\n",
    "    # if len(index)==1:\n",
    "    #     part_mean_shap=torch.tensor(np.mean(all_input_shap[index],axis=0))\n",
    "    # com_shap=y\n",
    "    # com_shap=torch.tensor(shaps_mean_pca[index])\n",
    "    # com_shap=torch.tensor(pca.inverse_transform(com_shap.detach().numpy()))\n",
    "    # print(all_shap[index].  shape)\n",
    "    if len(index)==1:\n",
    "        com_shap=torch.tensor(all_shap[index]).unsqueeze(0)\n",
    "    else:\n",
    "        com_shap=torch.tensor(all_shap[index])\n",
    "    # print(output_tensor.shape,com_shap.shape,part_mean_shap.shape)\n",
    "    # print(output_tensor.shape,com_shap.shape)\n",
    "    loss1+=torch.dist(output_tensor,com_shap,2)\n",
    "    loss2+=torch.dist(part_mean_shap,com_shap,2)\n",
    "    loss3+=torch.dist(output_tensor,part_mean_shap,2)\n",
    "    for i in range(len(index)):\n",
    "        loss4+=getDIff(200,output_tensor[i].detach().numpy(),com_shap[i].detach().numpy())\n",
    "        loss5+=getDIff(200,part_mean_shap[i].detach().numpy(),com_shap[i].detach().numpy())\n",
    "print(loss1,loss2,loss3,loss4,loss5)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
