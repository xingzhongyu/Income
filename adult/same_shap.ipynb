{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "file_names=[]\n",
    "all_shap=[]\n",
    "for i in range(140):\n",
    "    file_name=\"/home/zyxing/adult_income/shaps/clf\"+str(i)+\".ptl.npy\"\n",
    "    \n",
    "    try:\n",
    "        all_shap.append(np.load(file_name))\n",
    "        file_names.append(file_name)\n",
    "    except Exception as e:\n",
    "        continue\n",
    "leng=len(all_shap)\n",
    "all_shap=np.array(all_shap)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(94, 22792, 14)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_shap.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "#定义L1损失函数\n",
    "def L1_loss(y_true,y_pre): \n",
    "    return np.sum(np.abs(y_true-y_pre))\n",
    "#定义L2损失函数\n",
    "def L2_loss(y_true,y_pre):\n",
    "    return np.sum(np.square(y_true-y_pre))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import train_test_split\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# X_train, X_test, y_train, y_test = train_test_split(all_input_shap, shap_mean, test_size = 0.3, random_state = 0)\n",
    "# model= LinearRegression()\n",
    "\n",
    "# model.fit(X_train,y_train)\n",
    "\n",
    "# model.singular_=np.array([0.2]*5)\n",
    "# from sklearn.metrics import accuracy_score\n",
    "# y_pred=model.predict(X_test)\n",
    "#l2 148\n",
    "# L2_loss(y_pred,y_test)\n",
    "# np.sum(abs(y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset,DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "class MyDataSet(Dataset):\n",
    "    def __init__(self,data_x,data_y):\n",
    "        self.x=torch.tensor(data_x).to(torch.float)\n",
    "        self.y=torch.tensor(data_y).to(torch.float)\n",
    "        # print(self.x.shape,self.y.shape)\n",
    "    def __getitem__(self, item):\n",
    "        return self.x[item],self.y[item],item\n",
    "    def __len__(self):\n",
    "        return len(self.x)\n",
    "class MyNetWork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNetWork,self).__init__()\n",
    "        self.fc1=nn.Linear(5,1)\n",
    "        # self.fc2=nn.Linear(2,1)\n",
    "    def forward(self,x):\n",
    "        x=self.fc1(x)\n",
    "        # x=self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from numpy import int64\n",
    "\n",
    "df=pd.read_csv(\"adult.csv\",encoding='latin-1')\n",
    "df[df == '?'] = np.nan\n",
    "labels = df['income']\n",
    "labels=labels[:int(labels.shape[0]*0.7)].copy()\n",
    "labels[labels==\"<=50K\"]=0\n",
    "labels[labels==\">50K\"]=1\n",
    "label=labels.astype(int64)\n",
    "label=np.array(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "y_shaps=[]\n",
    "x_shaps=[]\n",
    "out_shaps=[]\n",
    "classes=[]\n",
    "def getLine(seed:int):\n",
    "    rand_ints=np.random.choice(leng,5,replace=False)\n",
    "    print(file_names[rand_ints])\n",
    "    all_input_shap=all_shap[rand_ints,:,:]\n",
    "    all_input_shap=all_input_shap.transpose([1,2,0])\n",
    "    shap_mean=np.mean(all_shap,axis=0)\n",
    "    # print(all_input_shap.shape,shap_mean.shape)\n",
    "    # print(all_shap.shape,all_input_shap.shape,shap_mean.shape)\n",
    "    if seed==-1:\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(all_input_shap, shap_mean, test_size = 0.3, shuffle=False)\n",
    "    else:\n",
    "        train_X, test_X, train_Y, test_Y = train_test_split(all_input_shap, shap_mean, test_size = 0.3, random_state=seed)\n",
    "    train_DataSet=MyDataSet(train_X,train_Y)\n",
    "    test_DataSet=MyDataSet(test_X,test_Y)\n",
    "    batch_size=3\n",
    "    print(train_X.shape)\n",
    "    train_dataloader = DataLoader(train_DataSet,batch_size=batch_size,shuffle=True)\n",
    "    test_dataloader = DataLoader(test_DataSet,batch_size=batch_size,shuffle=True)\n",
    "    net=MyNetWork()\n",
    "    sta_dic=net.state_dict()\n",
    "    sta_dic['fc1.weight']=torch.tensor([[0.2, 0.2, 0.2, 0.2, 0.2]])\n",
    "    sta_dic['fc1.bias']=torch.tensor([0])\n",
    "\n",
    "    net.load_state_dict(sta_dic)\n",
    "    import time\n",
    "    import torch.optim as optim\n",
    "    criterion=nn.L1Loss()\n",
    "    optimizer = optim.Adam(net.parameters(), lr=0.0001)\n",
    "    start = time.time()\n",
    "    for epoch in range(10):\n",
    "\n",
    "        running_loss = 0.0\n",
    "        for i, data in enumerate(train_dataloader, 0):\n",
    "            # 获取输入数据\n",
    "            inputs, label,_= data\n",
    "            # 清空梯度缓存\n",
    "            optimizer.zero_grad()\n",
    "            # print(inputs.shape)\n",
    "            # print(torch.dist(inputs,torch.tensor(partial_shaps[index]),1))\n",
    "            # inputs=inputs.transpose(1,2)\n",
    "            # print(inputs.shape)\n",
    "            outputs = net(inputs).squeeze()\n",
    "            label=label.squeeze()\n",
    "            # print(outputs.shape,label.shape)\n",
    "            loss = criterion(outputs, label)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # 打印统计信息\n",
    "            running_loss += loss.item()\n",
    "            # print(loss.item())\n",
    "    print('Finished Training! Total cost time: ', time.time()-start)\n",
    "    all_real_input_mean_shap=np.mean(all_input_shap,axis=2)\n",
    "    loss1=0\n",
    "    loss2=0\n",
    "    loss3=0\n",
    "    torch.save(net.state_dict(),\"./openXAI/net.pt\")\n",
    "    # print(net.state_dict())\n",
    "    for i,data in enumerate(test_dataloader,0):\n",
    "        x,y,index=data\n",
    "        index+=15954\n",
    "        # index+=train_size\n",
    "        # print(index)\n",
    "        # if len(index)==1:\n",
    "        #     classes.append(label[index.item()])\n",
    "        # else:\n",
    "        #     classes.append(label[index.items()])\n",
    "        # print([list(index.detach().numpy())])\n",
    "        classes.append(labels[list(index.detach().numpy())])\n",
    "        y=y.squeeze()\n",
    "        # x=x.transpose(1,2)\n",
    "        # print(torch.mean(x,dim=2).shape)\n",
    "        output_tensor=net.forward(x).squeeze()\n",
    "        all_real_input_mean_shap_np=np.array(torch.mean(x,dim=-1))\n",
    "        #5个模型求取平均代表的就是0.2\n",
    "        # print(output_tensor.shape,torch.tensor(all_real_mean_shap[index]).shape,torch.tensor(all_real_input_mean_shap[index]).shape)\n",
    "        # print(x.shape)\n",
    "        x_shaps.append(all_real_input_mean_shap_np)\n",
    "        y_shaps.append(y.detach().numpy())\n",
    "        out_shaps.append(output_tensor.detach().numpy())\n",
    "        # out_shaps.append(output_tensor.detach().numpy())\n",
    "        # y_shaps.append(y.detach().numpy())\n",
    "        loss1+=torch.dist(output_tensor,y,2)\n",
    "        loss2+=torch.dist(torch.tensor(all_real_input_mean_shap_np),y,2)\n",
    "        \n",
    "        # output_tensor_np=output_tensor.detach().numpy()\n",
    "        # y_np=y.detach().numpy()\n",
    "        \n",
    "        \n",
    "        # print(all_real_input_mean_shap_np.shape)\n",
    "        # if len(index)==1:\n",
    "        #     output_tensor_np=output_tensor_np[np.newaxis,:]\n",
    "        #     y_np=y_np[np.newaxis,:]\n",
    "            # all_real_input_mean_shap_np=all_real_input_mean_shap_np[np.newaxis,:]\n",
    "        # for i in range(len(index)):\n",
    "            \n",
    "        #     loss1+=getDIff(50,output_tensor_np[i],y_np[i])\n",
    "        #     loss2+=getDIff(50,all_real_input_mean_shap_np[i],y_np[i])\n",
    "    ans=str(loss1.item())+\"\\t\"+str(loss2.item())\n",
    "    print(ans)\n",
    "    \n",
    "    # model= LinearRegression()\n",
    "\n",
    "    # model.fit(train_X,train_Y)\n",
    "\n",
    "    # model.singular_=np.array([0.2]*5)\n",
    "    # pred_Y=model.predict(test_X)\n",
    "    # print(L2_loss(pred_Y,test_Y))\n",
    "\n",
    "    with open(\"same_deep\",\"a+\") as f:\n",
    "        f.write(ans+\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "only integer scalar arrays can be converted to a scalar index",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# for i in range(100):\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39m#     getLine(i)\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m getLine(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[23], line 10\u001b[0m, in \u001b[0;36mgetLine\u001b[0;34m(seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mgetLine\u001b[39m(seed:\u001b[39mint\u001b[39m):\n\u001b[1;32m      9\u001b[0m     rand_ints\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mchoice(leng,\u001b[39m5\u001b[39m,replace\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mprint\u001b[39m(file_names[rand_ints])\n\u001b[1;32m     11\u001b[0m     all_input_shap\u001b[39m=\u001b[39mall_shap[rand_ints,:,:]\n\u001b[1;32m     12\u001b[0m     all_input_shap\u001b[39m=\u001b[39mall_input_shap\u001b[39m.\u001b[39mtranspose([\u001b[39m1\u001b[39m,\u001b[39m2\u001b[39m,\u001b[39m0\u001b[39m])\n",
      "\u001b[0;31mTypeError\u001b[0m: only integer scalar arrays can be converted to a scalar index"
     ]
    }
   ],
   "source": [
    "# for i in range(100):\n",
    "#     getLine(i)\n",
    "getLine(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_shaps=np.concatenate(x_shaps,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_shaps[-1]=y_shaps[-1][np.newaxis,:]\n",
    "out_shaps[-1]=out_shaps[-1][np.newaxis,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_shaps=np.concatenate(y_shaps,axis=0)\n",
    "out_shaps=np.concatenate(out_shaps,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes=np.concatenate(classes,axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6838, 14) (6838, 14) (6838, 14) (6838,)\n"
     ]
    }
   ],
   "source": [
    "print(x_shaps.shape,y_shaps.shape,out_shaps.shape,classes.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getDis(data):\n",
    "    class0_shap=[]\n",
    "    class1_shap=[]\n",
    "    for label,data in zip(classes,data):\n",
    "        if label==0:\n",
    "            class0_shap.append(data)\n",
    "        elif label==1:\n",
    "            class1_shap.append(data)\n",
    "        else:\n",
    "            print(label)\n",
    "\n",
    "    class0_shap=np.array(class0_shap)\n",
    "    class1_shap=np.array(class1_shap)\n",
    "\n",
    "    return np.mean(np.std(class0_shap,axis=0)),np.mean(np.std(class1_shap,axis=0)),np.mean(np.std(data,axis=0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "def getGraph(datas,file_name):\n",
    "    G=nx.Graph()\n",
    "    for i,(label,data) in enumerate(zip(classes,datas)):\n",
    "        G.add_node(i,label=label,data=data)\n",
    "        for j in range(i):\n",
    "            G.add_edge(i,j,weights=L2_loss(G.nodes[i][\"data\"],G.nodes[j][\"data\"]))\n",
    "    nx.write_gpickle(G,file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getGraph(x_shaps,\"adult_x_shaos.gpickle\")\n",
    "# getGraph(y_shaps,\"adult_y_shaos.gpickle\")\n",
    "# getGraph(out_shaps,\"adult_out_shaos.gpickle\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "compare",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
